from nltk.tokenize import sent_tokenize, word_tokenize

Example_Text = "Hello, Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard."      # example text to be tokenized

print(word_tokenize(Example_Text))                                                                                                                                  # print a tokenization of the text (by word)
